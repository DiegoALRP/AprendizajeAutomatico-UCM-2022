{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09096e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "from pandas.io.parsers import read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.axes import Axes\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import math\n",
    "import scipy.optimize as opt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed80f5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_coste_reg(Thetas, X, Y, lmb):\n",
    "    m = np.shape(X)[0]\n",
    "\n",
    "    return cost_funct(Y, X, m) + regularizacion(Thetas, lmb, m)\n",
    "\n",
    "def sigmoid_funct(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_funct2(value):\n",
    "    s = sigmoid_funct(value)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def cost_funct(Y, g, m):\n",
    "    J = np.sum(-1*Y* np.log(g) -1*(1 - Y)* np.log(1-g))\n",
    "    return J/m\n",
    "\n",
    "def regularizacion(Thetas, lmb, m):\n",
    "    suma = 0\n",
    "    for i in Thetas:\n",
    "        i = i[:,1:]\n",
    "        suma += (np.sum(i**2))\n",
    "    \n",
    "    return (lmb/(2*m))*suma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e962de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, theta1, theta2, m, Y):\n",
    "    \n",
    "    #Capa entrada asignamos la X con los unos incluidos\n",
    "    a1 = np.hstack([np.ones([m, 1]), X])\n",
    "\n",
    "    #capa intermedia (hidden) calculamos las ecuaciones de la anterior, aplicamos la sigmoide e incluimos los unos de la neurona 0 \n",
    "    z2 = np.dot(a1, theta1.T)\n",
    "    a2 = np.hstack([np.ones([m, 1]), sigmoid_funct(z2)])\n",
    "\n",
    "    #Capa salida calcuamos las ecuaciones con theta2 y aplicamos la sigmoide, nos devuelve la matriz de salida 5000x10\n",
    "    z3 = np.dot(a2, theta2.T)\n",
    "    \n",
    "    a3 = sigmoid_funct(z3)\n",
    "    \n",
    "    return a1, z2, a2, z3, a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e74dabe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomWeights(entradas, salidas, ini):\n",
    "    Theta = np.random.uniform(-ini, ini, size = (entradas, salidas))\n",
    "    #Introducimos el 1\n",
    "    Theta = np.insert(Theta, 0, 1, axis = 0)\n",
    "    return Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75d0ff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def backprop(params_rn, num_entradas, num_ocultas, num_etiquetas, X, Y, reg):\n",
    "\n",
    "    Theta1 = np.reshape(params_rn[:num_ocultas *(num_entradas + 1)],(num_ocultas, (num_entradas+1)))\n",
    "    Theta2 = np.reshape(params_rn[num_ocultas*(num_entradas + 1): ], (num_etiquetas,(num_ocultas+1)))\n",
    "    \n",
    "    m = X.shape[0]    \n",
    "    y_onehot = Y\n",
    "    \n",
    "    a1, z2, a2, z3, a3 = forward_propagation(X, Theta1, Theta2, m, y_onehot)\n",
    "    \n",
    "    coste = func_coste_reg([Theta1, Theta2], a3, y_onehot, reg)\n",
    "   \n",
    "    g1 = np.zeros(Theta1.shape)\n",
    "    g2 = np.zeros(Theta2.shape)\n",
    "\n",
    "    d3 = np.array(a3 - y_onehot)\n",
    "    d2 = np.matmul(Theta2.T[1:, :],d3.T)*(sigmoid_funct2(z2.T))\n",
    "    \n",
    "    g1 = g1 + np.matmul(d2, a1)\n",
    "    g2 = g2 + np.matmul(d3.T,a2)\n",
    "    \n",
    "    G1 = g1/(m*1.0)\n",
    "    G2 = g2/(m*1.0)\n",
    "    \n",
    "    G1 = G1 + ((reg*1.0)/(m))*np.insert(Theta1[:, 1:], 0, 0, axis = 1)\n",
    "    G2 = G2 + ((reg*1.0)/(m))*np.insert(Theta2[:, 1:], 0, 0, axis = 1)\n",
    "\n",
    "    gradientes = np.concatenate((G1, G2), axis = None)\n",
    "    \n",
    "    return coste, gradientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35f1ed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizeNeuralNetwork(iterations, neuronas_ent, neuronas_oc, num_labels):\n",
    "    Theta1 = RandomWeights(neuronas_ent, neuronas_oc, 0.12)\n",
    "    Theta2 = RandomWeights(neuronas_oc, num_labels, 0.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "655185c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_grad_reg(Thetas, X, Y, lmb):\n",
    "    m = np.shape(X)[0]\n",
    "    return np.add(gradient(Thetas, X, Y) , reg_grad(Thetas[1:], lmb, m))\n",
    "\n",
    "def reg_grad(Thetas, lmb, m):\n",
    "    return np.insert((lmb/m)*Thetas, 0, values=[0])\n",
    "    \n",
    "def gradient(Theta, X, Y):\n",
    "    m = np.shape(X)[0]\n",
    "    g = sigmoid_funct(np.matmul(X,Theta))\n",
    "    J = np.dot(np.transpose(X), (g - Y))\n",
    "    return J/m \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7306a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c1769cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def debugInitializeWeights(fan_in, fan_out):\n",
    "    \"\"\"\n",
    "    Initializes the weights of a layer with fan_in incoming connections and\n",
    "    fan_out outgoing connections using a fixed set of values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set W to zero matrix\n",
    "    W = np.zeros((fan_out, fan_in + 1))\n",
    "\n",
    "    # Initialize W using \"sin\". This ensures that W is always of the same\n",
    "    # values and will be useful in debugging.\n",
    "    W = np.array([np.sin(w) for w in\n",
    "                  range(np.size(W))]).reshape((np.size(W, 0), np.size(W, 1)))\n",
    "\n",
    "    return W\n",
    "\n",
    "\n",
    "def computeNumericalGradient(J, theta):\n",
    "    \"\"\"\n",
    "    Computes the gradient of J around theta using finite differences and\n",
    "    yields a numerical estimate of the gradient.\n",
    "    \"\"\"\n",
    "\n",
    "    numgrad = np.zeros_like(theta)\n",
    "    perturb = np.zeros_like(theta)\n",
    "    tol = 1e-4\n",
    "\n",
    "    for p in range(len(theta)):\n",
    "        # Set perturbation vector\n",
    "        perturb[p] = tol\n",
    "        loss1 = J(theta - perturb)\n",
    "        loss2 = J(theta + perturb)\n",
    "\n",
    "        # Compute numerical gradient\n",
    "        numgrad[p] = (loss2 - loss1) / (2 * tol)\n",
    "        perturb[p] = 0\n",
    "\n",
    "    return numgrad\n",
    "\n",
    "\n",
    "def checkNNGradients(costNN, reg_param):\n",
    "    \"\"\"\n",
    "    Creates a small neural network to check the back propogation gradients.\n",
    "    Outputs the analytical gradients produced by the back prop code and the\n",
    "    numerical gradients computed using the computeNumericalGradient function.\n",
    "    These should result in very similar values.\n",
    "    \"\"\"\n",
    "    # Set up small NN\n",
    "    input_layer_size = 3\n",
    "    hidden_layer_size = 5\n",
    "    num_labels = 3\n",
    "    m = 5\n",
    "\n",
    "    # Generate some random test data\n",
    "    Theta1 = debugInitializeWeights(hidden_layer_size, input_layer_size)\n",
    "    Theta2 = debugInitializeWeights(num_labels, hidden_layer_size)\n",
    "\n",
    "    # Reusing debugInitializeWeights to get random X\n",
    "    X = debugInitializeWeights(input_layer_size - 1, m)\n",
    "\n",
    "    # Set each element of y to be in [0,num_labels]\n",
    "    y = [(i % num_labels) for i in range(m)]\n",
    "\n",
    "    ys = np.zeros((m, num_labels))\n",
    "    for i in range(m):\n",
    "        ys[i, y[i]] = 1\n",
    "\n",
    "    # Unroll parameters\n",
    "    nn_params = np.append(Theta1, Theta2).reshape(-1)\n",
    "\n",
    "    # Compute Cost\n",
    "    cost, grad = costNN(nn_params,\n",
    "                        input_layer_size,\n",
    "                        hidden_layer_size,\n",
    "                        num_labels,\n",
    "                        X, ys, reg_param)\n",
    "\n",
    "    def reduced_cost_func(p):\n",
    "        \"\"\" Cheaply decorated nnCostFunction \"\"\"\n",
    "        return costNN(p, input_layer_size, hidden_layer_size, num_labels,\n",
    "                      X, ys, reg_param)[0]\n",
    "\n",
    "    numgrad = computeNumericalGradient(reduced_cost_func, nn_params)\n",
    "\n",
    "    # Check two gradients\n",
    "    #print('grad shape: ', grad.shape)\n",
    "    #print('num grad shape: ', numgrad.shape)\n",
    "    np.testing.assert_almost_equal(grad, numgrad)\n",
    "    return (grad - numgrad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fe67b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_backprop_and_check (t1, t2, num_entradas, num_ocultas, num_etiquetas, reg, X, y_onehot, laps, Y):\n",
    "    \n",
    "    #Theta1 = RandomWeights(num_entradas, num_ocultas, 0.12)\n",
    "    #Theta2 = RandomWeights(num_ocultas, num_etiquetas, 0.12)\n",
    "    \n",
    "    #pesos = np.concatenate((Theta1, Theta2), axis=None)\n",
    "    eini = 0.12\n",
    "    params = np.concatenate([np.ravel(t1),np.ravel(t2)])\n",
    "    pesos = np.random.uniform(-eini,eini, params.shape[0])\n",
    "\n",
    "    out = minimize(fun = backprop, x0 = pesos, args = (num_entradas, num_ocultas, num_etiquetas, X, y_onehot, reg), method='TNC', jac = True, options = {'maxiter': laps})\n",
    "\n",
    "    Theta1 = out.x[:(num_ocultas*(num_entradas+1))].reshape(num_ocultas,(num_entradas+1))\n",
    "    Theta2 = out.x[(num_ocultas*(num_entradas+1)):].reshape(num_etiquetas,(num_ocultas+1))\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    a1, z2, a2, z3, a3 = forward_propagation(X, Theta1, Theta2, m, y_onehot)\n",
    "    \n",
    "    indexes = np.argmax(a3, axis=1)\n",
    "    acc = (np.sum(indexes == (Y))/m)*100\n",
    "    #print(\"Porcentaje acertados:\", acc, \"%\")\n",
    "    \n",
    "    print(acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb6bffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    data = loadmat (\"ex4data1.mat\")\n",
    "\n",
    "    #almacenamos los datos leídos en X e y\n",
    "    X = data['X']\n",
    "    m = X.shape[0]\n",
    "    #X = np.hstack([np.ones([m, 1]), X])\n",
    "    y = data['y']\n",
    "    y = np.ravel(y) \n",
    "    num_labels = 10\n",
    "    num_entries = np.shape(X)[1]\n",
    "    num_hiden_layers = 25\n",
    "\n",
    "    y = y-1\n",
    "    y_onehot = np.zeros((m, num_labels))  # 5000 x 10\n",
    "    \n",
    "    for i in range(m):\n",
    "        y_onehot[i][y[i]] = 1\n",
    "    \n",
    "    weights = loadmat(\"ex4weights.mat\")\n",
    "    theta1, theta2 = weights[\"Theta1\"], weights[\"Theta2\"]\n",
    "    #print(np.shape(theta1))\n",
    "    #print(np.shape(theta2))\n",
    "    \n",
    "    a1, z2, a2, z3, a3 = forward_propagation(X, theta1, theta2, m, y_onehot)\n",
    "        \n",
    "    #indexes = np.argmax(h, axis=1)\n",
    "    \n",
    "    reg = 1\n",
    "    \n",
    "    #print(checkNNGradients ( backprop ,  reg))\n",
    "    \n",
    "    acc = optimize_backprop_and_check(theta1, theta2, num_entries, num_hiden_layers, num_labels, reg, X, y_onehot, 70, y)\n",
    "\n",
    "    return acc\n",
    "    #indexes = np.argmax(a3, axis=1)\n",
    "    #acc = (np.sum(indexes == y)/m)*100\n",
    "    #print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "516e5d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.44\n",
      "91.74\n",
      "91.24\n",
      "91.97999999999999\n",
      "90.94\n",
      "92.86\n",
      "92.75999999999999\n",
      "93.86\n",
      "93.0\n",
      "93.2\n",
      "91.24\n",
      "92.94\n",
      "92.06\n",
      "92.16\n",
      "93.17999999999999\n",
      "91.86\n",
      "92.44\n",
      "93.78\n",
      "93.58\n",
      "90.3\n",
      "92.328\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "for i in range(0, 20):\n",
    "    acc = acc + main()\n",
    "acc = acc/20\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9874ff9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
